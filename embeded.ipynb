{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bpe import HindiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"hindi_gpt/hindi.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = HindiTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hindiDataset:\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    from bpe import HindiTokenizer\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size, max_length, stride, shuffle=True, drop_last = True, num_workers=0):\n",
    "\n",
    "    dataset = hindiDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size, shuffle, drop_last=drop_last, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 653, 6861,   96, 5065]]), tensor([[6861,   96, 5065, 6861]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[  96, 5065, 6861, 2303]])\n",
      "Target:  tensor([[5065, 6861, 2303, 4800]])\n"
     ]
    }
   ],
   "source": [
    "inputs1, targets1 = next(data_iter)\n",
    "print(\"Input: \", inputs1)\n",
    "print(\"Target: \", targets1)from bpe import HindiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100863\n",
    "output_dim = 256\n",
    "context_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[  653,  6861,    96,  5065],\n",
      "        [ 6861,  2303,  4800,    90],\n",
      "        [ 6569, 33974,   693,   102],\n",
      "        [  164,  1033,   124, 90890],\n",
      "        [ 1469,   102, 12280,    95],\n",
      "        [ 1139,    90,   574,  9889],\n",
      "        [   97,   237, 17159,  8032],\n",
      "        [   90,   473,    97,  8953]])\n",
      "Target:  tensor([[ 6861,    96,  5065,  6861],\n",
      "        [ 2303,  4800,    90,  6569],\n",
      "        [33974,   693,   102,   164],\n",
      "        [ 1033,   124, 90890,  1469],\n",
      "        [  102, 12280,    95,  1139],\n",
      "        [   90,   574,  9889,    97],\n",
      "        [  237, 17159,  8032,    90],\n",
      "        [  473,    97,  8953,    95]])\n",
      "Input shape:  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Input: \", inputs)\n",
    "print(\"Target: \", targets)\n",
    "print(\"Input shape: \", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1801,  0.5531, -1.2463,  ...,  0.5887, -0.4320, -0.5118],\n",
      "         [ 0.9121, -0.1160, -1.2231,  ...,  0.3986, -0.7128,  0.3800],\n",
      "         [-1.6821,  0.3227, -0.4879,  ..., -1.2016, -0.8116, -0.7672],\n",
      "         [-0.1796, -0.1454,  0.3370,  ...,  0.1366,  0.0972, -1.1325]],\n",
      "\n",
      "        [[ 0.9121, -0.1160, -1.2231,  ...,  0.3986, -0.7128,  0.3800],\n",
      "         [ 1.1497,  0.0048,  1.1323,  ..., -0.5259,  0.5716,  0.2707],\n",
      "         [ 0.8521, -1.3983, -1.0890,  ..., -0.0237, -0.0908, -0.2665],\n",
      "         [ 1.6777,  0.2427, -0.8827,  ...,  0.3661,  0.1276, -0.2763]],\n",
      "\n",
      "        [[-1.0159,  0.9452, -0.4051,  ..., -0.0180, -1.3032, -1.1961],\n",
      "         [-0.9672,  0.6358,  1.8355,  ..., -0.2647,  0.2990,  0.6080],\n",
      "         [-0.2828,  1.0133, -0.3841,  ...,  1.8431,  0.5580,  0.9344],\n",
      "         [ 0.2710, -0.0894,  0.6879,  ...,  0.5631, -0.7318,  1.3994]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0804,  0.7463,  2.2777,  ..., -0.8547,  1.1046,  2.4741],\n",
      "         [ 1.6777,  0.2427, -0.8827,  ...,  0.3661,  0.1276, -0.2763],\n",
      "         [-2.7722, -0.7168,  0.9230,  ..., -0.7344,  1.7699,  0.2488],\n",
      "         [ 1.6195,  0.5057, -2.0980,  ..., -2.3964,  0.9766, -0.3575]],\n",
      "\n",
      "        [[-0.9805, -1.6354, -0.1739,  ...,  0.7410,  0.5042, -0.6326],\n",
      "         [ 1.1749,  0.5317,  0.5181,  ...,  0.0748,  0.0220, -0.4636],\n",
      "         [-2.3964, -0.7214, -1.0126,  ...,  1.2944, -0.7751, -1.8336],\n",
      "         [-0.7600, -0.4689, -0.9284,  ...,  1.5126,  0.3836, -0.9690]],\n",
      "\n",
      "        [[ 1.6777,  0.2427, -0.8827,  ...,  0.3661,  0.1276, -0.2763],\n",
      "         [-1.0645,  1.5676, -1.2156,  ...,  0.5403, -0.9713, -0.9338],\n",
      "         [-0.9805, -1.6354, -0.1739,  ...,  0.7410,  0.5042, -0.6326],\n",
      "         [-0.9445, -0.2626,  0.4082,  ...,  1.2245, -1.1998,  2.6012]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
