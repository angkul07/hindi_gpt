{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bpe import HindiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/angkul/my_data/coding/agi/hindi_gpt/hindi.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = HindiTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hindiDataset:\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size, max_length, stride, shuffle=True, drop_last = True, num_workers=0):\n",
    "\n",
    "    dataset = hindiDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size, shuffle, drop_last=drop_last, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 661, 6735,   99, 5097]]), tensor([[6735,   99, 5097, 6735]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=1, max_length=4, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[  99, 5097, 6735, 2311]])\n",
      "Target:  tensor([[5097, 6735, 2311, 4831]])\n"
     ]
    }
   ],
   "source": [
    "inputs1, targets1 = next(data_iter)\n",
    "print(\"Input: \", inputs1)\n",
    "print(\"Target: \", targets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 110126\n",
    "output_dim = 256\n",
    "context_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[  661,  6735,    99,  5097],\n",
      "        [ 6735,  2311,  4831,    93],\n",
      "        [ 6639, 34529,   704,   104],\n",
      "        [  167,  1041,   127, 98689],\n",
      "        [ 1456,   104, 12385,    98],\n",
      "        [ 1056,    93,   577,  9970],\n",
      "        [  100,   240, 17327,  8104],\n",
      "        [   93,   480,   100,  9039]])\n",
      "Target:  tensor([[ 6735,    99,  5097,  6735],\n",
      "        [ 2311,  4831,    93,  6639],\n",
      "        [34529,   704,   104,   167],\n",
      "        [ 1041,   127, 98689,  1456],\n",
      "        [  104, 12385,    98,  1056],\n",
      "        [   93,   577,  9970,   100],\n",
      "        [  240, 17327,  8104,    93],\n",
      "        [  480,   100,  9039,    98]])\n",
      "Input shape:  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Input: \", inputs)\n",
    "print(\"Target: \", targets)\n",
    "print(\"Input shape: \", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0748,  0.8920,  0.3683,  ..., -0.9974, -2.5803, -1.4525],\n",
      "         [ 0.9381,  0.2983, -0.9376,  ...,  2.7328,  0.7609, -1.6878],\n",
      "         [-0.0162, -1.1155,  0.9063,  ..., -0.7967, -0.1410,  0.0396],\n",
      "         [ 1.2758,  0.6764,  0.0542,  ..., -0.6666, -1.3848, -0.3094]],\n",
      "\n",
      "        [[ 0.9381,  0.2983, -0.9376,  ...,  2.7328,  0.7609, -1.6878],\n",
      "         [ 0.4075, -0.6495, -1.1585,  ..., -0.2722, -1.1590,  0.6773],\n",
      "         [ 0.4540, -1.0890, -1.3192,  ..., -1.3655, -0.7506,  0.4113],\n",
      "         [ 0.2202,  1.1878,  0.1690,  ..., -0.4608,  1.3009, -1.5901]],\n",
      "\n",
      "        [[ 0.6723,  0.9529,  1.3828,  ..., -1.0226,  0.3758,  0.3551],\n",
      "         [ 0.1772, -0.6796, -0.5324,  ..., -1.2889,  0.2810, -0.5061],\n",
      "         [-0.4776, -0.2462, -0.7918,  ...,  0.9150, -0.0212, -1.4770],\n",
      "         [ 1.2324,  0.3857,  0.2084,  ...,  1.0293,  2.8439,  1.4019]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7395,  0.3440, -0.6195,  ..., -0.2009,  0.1521, -1.1817],\n",
      "         [ 0.2202,  1.1878,  0.1690,  ..., -0.4608,  1.3009, -1.5901],\n",
      "         [-1.2437, -2.3163,  1.6590,  ..., -0.3735, -0.8194, -1.5915],\n",
      "         [ 0.2756,  0.4572, -0.9086,  ...,  0.2101, -0.2022, -0.8359]],\n",
      "\n",
      "        [[-0.1237,  0.5201,  0.3674,  ..., -1.9582, -0.6576,  0.4034],\n",
      "         [ 0.7345, -1.2351,  0.4666,  ..., -0.5238,  0.0782,  0.0423],\n",
      "         [-0.5111,  0.2755,  3.0902,  ...,  1.4881,  0.6544,  1.0044],\n",
      "         [ 0.6856,  2.2314,  0.0821,  ...,  1.7817,  0.2000,  2.3361]],\n",
      "\n",
      "        [[ 0.2202,  1.1878,  0.1690,  ..., -0.4608,  1.3009, -1.5901],\n",
      "         [ 0.9899,  1.4364,  0.0153,  ...,  0.7943, -1.4882, -0.0276],\n",
      "         [-0.1237,  0.5201,  0.3674,  ..., -1.9582, -0.6576,  0.4034],\n",
      "         [-0.9912, -0.3919, -0.2051,  ..., -1.1645,  1.0262,  0.2927]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
